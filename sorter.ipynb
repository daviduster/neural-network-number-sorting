{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    '''\n",
    "    Function to return softmax\n",
    "    '''\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (np.array(softmax([1,2,3])) != np.array([0.09003057, 0.24472847, 0.66524096])).all(), \"Softmax result not correct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(self, prob, target):\n",
    "    '''\n",
    "    Calculate cross-entrpy loss\n",
    "    '''\n",
    "    return -np.sum(np.log(prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, max_n, hidden_layers):\n",
    "        '''\n",
    "        W is weight\n",
    "        Whh weight at previous hidden state\n",
    "        Whx weight at current input state\n",
    "        Why weight at the output state\n",
    "\n",
    "        Formula for the current state: h_t = f(h_{t-1}, x_t) where \n",
    "\n",
    "        '''\n",
    "        self.max_n = max_n\n",
    "        self.hidden_layers = hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(self, x_input, y, size):\n",
    "    \n",
    "    hidden_states = []\n",
    "    hidden_states.append(np.zeros((self.hidden_layers, 1)))    \n",
    "    \n",
    "    probs = []\n",
    "    prediction = np.zeros(size)\n",
    "    loss = 0\n",
    "    \n",
    "    for pos in range(size):\n",
    "        x_input[self.max_n] = pos\n",
    "        hidden = np.tanh(np.dot(self.Wxh, x_input) + np.dot(self .Whh, hidden_states[-1]))\n",
    "        hidden_states.append(hidden)\n",
    "\n",
    "        output = np.dot(self.Why, hidden)\n",
    "        prob = softmax(output)\n",
    "        probs.append(prob)\n",
    "\n",
    "        loss += -np.log(prob[y[pos],0]) \n",
    "\n",
    "        prediction[pos] = np.argmax(prob)\n",
    "            \n",
    "    return prediction, hidden_states, probs, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def rnn_backward(self, x_input, y, size, hidden_states, probs):\n",
    "\n",
    "        dWxh = np.zeros_like(self.Wxh)\n",
    "        dWhh = np.zeros_like(self.Whh)\n",
    "        dWhy = np.zeros_like(self.Why)\n",
    "        dhnext np.zeros_like(hidden_states[0])\n",
    "        \n",
    "        for pos in reversed(range(size)):\n",
    "            X[self.max_n] = pos\n",
    "            dy = np.copy(probs[pos])\n",
    "            dy[y[pos]] -= 1 \n",
    "\n",
    "            dWhy += np.dot(dy, hidden_states[pos].T)\n",
    "            \n",
    "            dh = np.dot(self.Why.T, dy) + dhnext \n",
    "            dhraw = (1 - hidden_states[pos] * hidden_states[pos]) * dh \n",
    "            \n",
    "            dWxh += np.dot(dhraw, X.T)\n",
    "            dWhh += np.dot(dhraw, hidden_states[pos-1].T)\n",
    "            \n",
    "            dhnext = np.dot(self.Whh.T, dhraw)\n",
    "\n",
    "            \n",
    "        return dWxh, dWhh, dWhy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def train(self, max_examples, learning_rate, max_seq_len:, lr_lambda = 0.4, lr_reduce_rate = 500, info_rate = 100):\n",
    "        \n",
    "        distances = 0\n",
    "        dist_list = []\n",
    "        \n",
    "        for i in range(max_examples):\n",
    "            sequence_size = max_seq_len\n",
    "            \n",
    "            X_input = np.random.randint(self.max_n, size=sequence_size)\n",
    "            y = np.sort(X_input)\n",
    "\n",
    "            one_hot = np.zeros((self.max_n + 1, 1))\n",
    "            one_hot[X_input, :] = 1\n",
    "        \n",
    "            prediction, hidden_states, probs, loss = self.rnn_forward(one_hot, y, sequence_size)\n",
    "                \n",
    "            distances += levendistance(prediction, y)\n",
    "            \n",
    "            dWxh, dWhh, dWhy = self.backward(one_hot, y, sequence_size, \n",
    "                                        hidden_states, probs)\n",
    "            \n",
    "            self.Wxh += -learning_rate * dWxh\n",
    "            self.Whh += -learning_rate * dWhh\n",
    "            self.Why += -learning_rate * dWhy\n",
    "            \n",
    "            if (i + 1) % lr_reduce_rate == 0:\n",
    "                learning_rate *= lr_lambda\n",
    "\n",
    "            if (i + 1) % info_rate == 0:\n",
    "                average_distance: float = float(distances) / info_rate\n",
    "                dist_list.append(average_distance)\n",
    "                print('Levenshtein distance for last {} sequences = {}'.format(info_rate, average_distance))\n",
    "                \n",
    "        return dist_list"
   ]
  }
 ]
}