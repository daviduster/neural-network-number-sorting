{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('env')",
   "metadata": {
    "interpreter": {
     "hash": "e6525fb63fcf59715cc49c98de881a73e19dac34a524e7dbd2bcd153111d62c9"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "source": [
    "# Encoder-Decoder LSTM for Sorting Numbers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The first step is to configure the problem, we will use with 6 numbers to order."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 50\n",
    "n_steps_in = 6\n",
    "n_steps_out = 6"
   ]
  },
  {
   "source": [
    "Function to define an encoder-decoder recurrent neural networks, two recurrent neural networks, one to encode the source sequence (encoder), and a second to decode the encoded source sequence into the target sequence (decoder).\n",
    "\n",
    "For training the model takes both the input and a shifted version of the target sequence as input and predicts the whole target sequence, the inference encoder model is used to encode the input sequence once which returns states that are used to initialize the inference decoder model. From that point, the inference decoder model is used to generate predictions step by step."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns train, inference_encoder and inference_decoder models\n",
    "def models(n_input, n_output, n_units):\n",
    "    \"\"\"\n",
    "    Create models for encoder-decoder neural network\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_input : int\n",
    "        Length of input sequence, e.g number of integers to sort\n",
    "    n_output : int\n",
    "        Length of output sequence, e.g number of integers sorted\n",
    "    n_units : int\n",
    "        Cells to create in the encoder and decoder models\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model, encoder_model, decoder_model\n",
    "    \"\"\"\n",
    "    # training encoder\n",
    "    encoder_inputs = Input(shape=(None, n_input))\n",
    "    encoder = LSTM(n_units, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    \n",
    "    # training decoder\n",
    "    decoder_inputs = Input(shape=(None, n_output))\n",
    "    decoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "    decoder_dense = Dense(n_output, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "    # inference encoder\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    # inference decoder\n",
    "    decoder_state_input_h = Input(shape=(n_units,))\n",
    "    decoder_state_input_c = Input(shape=(n_units,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "source": [
    "Defining the models and compiling the training model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, encoder, decoder = models(n_features, n_features, 128)\n",
    "train.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "source": [
    "Generate a training dataset and traing the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(length, max_number):\n",
    "    \"\"\"\n",
    "    Generate a sequence of random numbers\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    length : int\n",
    "        Length of the sequence\n",
    "    max_number : int\n",
    "        Maximum number value\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sequence\n",
    "    \"\"\"\n",
    "    sequence = np.random.randint(max_number, size=length)\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_decode(encoded_seq):\n",
    "    \"\"\"\n",
    "    Decode a one hot encoded string\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    encoded_seq : int\n",
    "        Length of the sequence\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sequence decoded\n",
    "    \"\"\"\n",
    "    return [np.argmax(vector) for vector in encoded_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(n_input, max_number, n_samples):\n",
    "    \"\"\"\n",
    "    Generate a number of sequences to use to train a model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_input : int\n",
    "        Length of input sequence, e.g number of integers to sort\n",
    "    max_number : int\n",
    "        Maximum number value\n",
    "    n_samples : int\n",
    "        Number of samples to generate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X1, X2, y\n",
    "        Train and target datasets\n",
    "    \"\"\"\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    for _ in range(n_samples):\n",
    "        # generate source sequence\n",
    "        source = generate_sequence(n_input, max_number)\n",
    "        # define target sequence\n",
    "        target = sorted(source)\n",
    "        # create padded input target sequence\n",
    "        target_in = [0] + target[:-1]\n",
    "        # encode\n",
    "        src_encoded = to_categorical(source, num_classes=max_number)\n",
    "        tar_encoded = to_categorical(target, num_classes=max_number)\n",
    "        tar2_encoded = to_categorical(target_in, num_classes=max_number)\n",
    "        # store\n",
    "        X1.append(src_encoded)\n",
    "        X2.append(tar2_encoded)\n",
    "        y.append(tar_encoded)\n",
    "\n",
    "    return np.array(X1), np.array(X2), np.array(y)"
   ]
  },
  {
   "source": [
    "Example of dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1, 6, 50) (1, 6, 50) (1, 6, 50)\nX1=[12, 33, 7, 5, 18, 46], X2=[0, 5, 7, 12, 18, 33], target=[5, 7, 12, 18, 33, 46]\n"
     ]
    }
   ],
   "source": [
    "n_samples = 1\n",
    "X1, X2, y = generate_dataset(n_steps_in, n_features, n_samples)\n",
    "print(X1.shape, X2.shape, y.shape)\n",
    "print('X1=%s, X2=%s, target=%s' % (one_hot_decode(X1[0]), one_hot_decode(X2[0]), one_hot_decode(y[0])))"
   ]
  },
  {
   "source": [
    "Generate a training dataset of 50000 examples and train the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(50000, 6, 50) (50000, 6, 50) (50000, 6, 50)\n"
     ]
    }
   ],
   "source": [
    "n_samples = 50000\n",
    "X1, X2, y = generate_dataset(n_steps_in, n_features, n_samples)\n",
    "print(X1.shape, X2.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.7174 - accuracy: 0.8339\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x136e4f760>"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "train.fit([X1, X2], y, epochs=1)"
   ]
  },
  {
   "source": [
    "After the model is trained, we can evaluate it, we define a function to generate a target sequence given a sequence"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(encoder, decoder, source, n_steps, max_number):\n",
    "    \"\"\"\n",
    "    Predict target sequence given \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    encoder : model\n",
    "        Encoder when making a prediction for a sequence\n",
    "    decoder : model\n",
    "        Decoder when making a prediction for a sequence\n",
    "    source : array\n",
    "        Encoded sequence\n",
    "    n_steps: int\n",
    "        Number of numbers to predict\n",
    "    max_number : int\n",
    "        Maximum number value for reshape\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list with the target sequence predicted\n",
    "    \"\"\"\n",
    "    # encode\n",
    "    state = encoder.predict(source)\n",
    "    # start of sequence input\n",
    "    target_seq = np.array([0.0 for _ in range(max_number)]).reshape(1, 1, max_number)\n",
    "    \n",
    "    # collect predictions\n",
    "    output = list()\n",
    "    for t in range(n_steps):\n",
    "        # predict next char\n",
    "        yhat, h, c = decoder.predict([target_seq] + state)\n",
    "        # store prediction\n",
    "        output.append(yhat[0,0,:])\n",
    "        # update state\n",
    "        state = [h, c]\n",
    "        # update target sequence\n",
    "        target_seq = yhat\n",
    "    return np.array(output)"
   ]
  },
  {
   "source": [
    "We evaluate our lstm model by making predictions for 100 sequences and counting the number of sequences we predicted correctly, we can see that with 50000 number of samples we got an accuracy over 90%!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 96.00%\n"
     ]
    }
   ],
   "source": [
    "total, correct = 100, 0\n",
    "for _ in range(total):\n",
    "    X1, X2, y = generate_dataset(n_steps_in, n_features, 1)\n",
    "    target = predict_sequence(encoder, decoder, X1, n_steps_out, n_features)\n",
    "    if np.array_equal(one_hot_decode(y[0]), one_hot_decode(target)):\n",
    "        correct += 1\n",
    "print('Accuracy: %.2f%%' % (float(correct)/float(total)*100.0))"
   ]
  },
  {
   "source": [
    "Finally, print the input, target and predictions of 10 examples to see how the model is working!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input=[0, 36, 33, 25, 21, 26] target=[0, 21, 25, 26, 33, 36], prediction=[0, 21, 25, 26, 33, 36]\n",
      "Input=[5, 23, 41, 44, 11, 38] target=[5, 11, 23, 38, 41, 44], prediction=[5, 11, 23, 38, 41, 44]\n",
      "Input=[0, 19, 34, 16, 22, 15] target=[0, 15, 16, 19, 22, 34], prediction=[0, 15, 16, 19, 22, 34]\n",
      "Input=[42, 21, 29, 30, 44, 11] target=[11, 21, 29, 30, 42, 44], prediction=[11, 21, 29, 30, 42, 44]\n",
      "Input=[45, 22, 17, 7, 29, 26] target=[7, 17, 22, 26, 29, 45], prediction=[7, 17, 22, 26, 29, 45]\n",
      "Input=[37, 12, 6, 7, 27, 10] target=[6, 7, 10, 12, 27, 37], prediction=[6, 7, 10, 12, 27, 37]\n",
      "Input=[8, 49, 19, 25, 35, 32] target=[8, 19, 25, 32, 35, 49], prediction=[8, 19, 25, 32, 35, 49]\n",
      "Input=[36, 38, 2, 28, 38, 23] target=[2, 23, 28, 36, 38, 38], prediction=[2, 23, 28, 38, 38, 38]\n",
      "Input=[8, 22, 35, 3, 6, 21] target=[3, 6, 8, 21, 22, 35], prediction=[3, 6, 8, 21, 22, 35]\n",
      "Input=[15, 12, 34, 19, 5, 25] target=[5, 12, 15, 19, 25, 34], prediction=[5, 12, 15, 19, 25, 34]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "\tX1, X2, y = generate_dataset(n_steps_in, n_features, 1)\n",
    "\ttarget = predict_sequence(encoder, decoder, X1, n_steps_out, n_features)\n",
    "\tprint('Input=%s target=%s, prediction=%s' % (one_hot_decode(X1[0]), one_hot_decode(y[0]), one_hot_decode(target)))"
   ]
  }
 ]
}